{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8fcf34-ed8e-42ee-8b8a-9bbea4994693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor suggested subtasks:\n",
      "No subtasks needed for this question! The answer can be calculated directly:\n",
      "\n",
      "No subtasks.\n",
      "\n",
      "Final Answer:\n",
      "As a supervisor agent, I will review the worker's results and provide the final answer.\n",
      "\n",
      "Since the worker's results are [], it means they have not provided any calculation or result. Therefore, I will perform the calculation myself to provide the correct answer.\n",
      "\n",
      "7 multiplied by 6 is:\n",
      "\n",
      "7 Ã— 6 = 42\n",
      "\n",
      "So, the final answer is: 42\n"
     ]
    }
   ],
   "source": [
    "#Task 17 : LangGraph - day5 \n",
    "#Implementing Supervisor agent and Swarms\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "import re\n",
    "\n",
    "# -------------------------\n",
    "#  Base LLM & Tool\n",
    "# -------------------------\n",
    "\n",
    "# Connect to llama3\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "# Define tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# -------------------------\n",
    "#  Worker Agent\n",
    "# -------------------------\n",
    "\n",
    "def worker_agent(task_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Worker agent:\n",
    "    - Invokes LLM to reason\n",
    "    - Executes multiply if requested\n",
    "    \"\"\"\n",
    "    response = llm.invoke(task_prompt).content\n",
    "    \n",
    "    # Parse tool call\n",
    "    if \"Action:\" in response:\n",
    "        numbers = re.findall(r\"\\d+\", response)\n",
    "        if len(numbers) >= 2:\n",
    "            result = multiply(int(numbers[0]), int(numbers[1]))\n",
    "            return str(result)\n",
    "    return response\n",
    "\n",
    "# -------------------------\n",
    "#  Supervisor Agent\n",
    "# -------------------------\n",
    "\n",
    "def supervisor_agent(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Supervisor agent:\n",
    "    - Decides subtasks\n",
    "    - Sends subtasks to worker swarm\n",
    "    - Aggregates worker results\n",
    "    \"\"\"\n",
    "    # Step 1: Ask supervisor to split tasks\n",
    "    prompt = f\"\"\"\n",
    "You are a supervisor agent. Decide if this question needs subtasks.\n",
    "Question: {question}\n",
    "\n",
    "Respond in the format:\n",
    "Subtasks:\n",
    "1: <task description>\n",
    "2: <task description>\n",
    "If only one task, just list 1.\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    print(\"Supervisor suggested subtasks:\")\n",
    "    print(response)\n",
    "    \n",
    "    # Step 2: Extract tasks\n",
    "    tasks = re.findall(r\"\\d+: (.+)\", response)\n",
    "    \n",
    "    # Step 3: Send tasks to workers (swarm)\n",
    "    worker_results = [worker_agent(task) for task in tasks]\n",
    "    \n",
    "    # Step 4: Aggregate\n",
    "    final_prompt = f\"\"\"\n",
    "You are a supervisor agent.\n",
    "The original question was: {question}\n",
    "The worker results are: {worker_results}\n",
    "Provide the final answer.\n",
    "\"\"\"\n",
    "    final_response = llm.invoke(final_prompt).content\n",
    "    return final_response\n",
    "\n",
    "# -------------------------\n",
    "#  Run Example\n",
    "# -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is 7 multiplied by 6?\"\n",
    "    final_answer = supervisor_agent(question)\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe516e20-371c-4e17-87fe-6a30fc64d81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
